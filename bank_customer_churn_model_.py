# -*- coding: utf-8 -*-
"""Bank Customer Churn Model .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EsRwiXKS0Cm6ZNS3jC72bfXA_y4e7QJP

# **Bank Customer Churn Model**

# **Objective**
We aim to accomplist the following for this study:

1] Identify and visualize which factors contribute to customer churn:

2] Build a prediction model that will perform the following:

   *   Classify if a customer is going to churn or not
   *   Preferably and based on model performance, choose a model that will attach a probability to the churn to make it easier for customer service to target low hanging fruits in their efforts to prevent churn

# **Data Source**
https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv
"""

# @title Import Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# @title Import Data
bccm = pd.read_csv('https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')

bccm.head()

bccm.info()

bccm.shape

# @title Describe Data
bccm.describe()

bccm.dtypes

bccm['Balance']

print(bccm.describe())
print("\n \n Shape of the data :", bccm.shape)
print("\n \n The info of the data :", bccm.info)

bccm.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')

bccm.describe(include=object).T

# @title Data Visualization
bccm.columns

bccm = pd.read_csv('https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')
bccm.columns =('CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age',
       'Tenure', 'Balance', 'Num Of Products', 'Has Credit Card',
       'Is Active Member', 'Estimated Salary', 'Churn')

str(bccm)

object_columns = bccm.select_dtypes(include='object').columns
df_numeric = bccm.drop(columns=object_columns)
matrix = np.triu(df_numeric.corr())
plt.figure(figsize=(14, 10))
sns.heatmap(df_numeric.corr(), annot=True, cmap=sns.cubehelix_palette(8), mask=matrix)
plt.xticks(rotation=45);

bccm.columns

# @title Surname
bccm["Surname"].value_counts()

bccm["Surname"].value_counts()

def first_look(column_name):
  print(bccm[column_name].value_counts())

first_look("Surname")

bccm["Surname"].describe().T

bccm.columns

def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 6))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.show()

categorical = ['Geography', 'Gender', 'Num Of Products', 'Has Credit Card', 'Is Active Member', 'Age','Tenure','Churn']
for i in categorical:
    print(df.columns)
    stacked_barplot(bccm,i,'Churn')

# @title Data Preprocessing
df.isna().sum()

df.nunique()

bccm.columns

# @title Define Target Variable (y) and Feature Variables (X)
X = bccm.drop('CustomerId',axis=1)
y = bccm['Churn']

dummy_cat = ['Geography','Gender']
X = pd.get_dummies(X,columns=dummy_cat,drop_first= True)
X.head()

df['Churn'].value_counts()

#  @title Train Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, stratify = y, random_state = 2529)

print(f''' X_train shape: {X_train.shape}
 X_test shape: {X_test.shape}
 y_train shape: {y_train.shape}
 y_test shape: {y_test.shape}''')

from imblearn.over_sampling import SMOTE

non_numeric_columns = X_train.select_dtypes(exclude=['number']).columns


X_train = pd.get_dummies(X_train, columns=non_numeric_columns)
X_test = pd.get_dummies(X_test, columns=non_numeric_columns)

from imblearn.over_sampling import SMOTE

print("Before UpSampling, counts of label 'Yes': {}".format(sum(y_train == 1)))
print("Before UpSampling, counts of label 'No': {} \n".format(sum(y_train == 0)))

sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)

X_train_over, y_train_over = sm.fit_resample(X_train, y_train)

print("After UpSampling, counts of label 'Yes': {}".format(sum(y_train_over == 1)))
print("After UpSampling, counts of label 'No': {} \n".format(sum(y_train_over == 0)))


print("After UpSampling, the shape of X_train: {}".format(X_train_over.shape))
print("After UpSampling, the shape of y-train: {} \n".format(y_train_over.shape))

from imblearn.under_sampling import RandomUnderSampler

random_us = RandomUnderSampler(random_state=1, sampling_strategy = 0.5)
X_train_un, y_train_un = random_us.fit_resample(X_train, y_train)

print("After UpSampling, counts of label 'Yes': {}".format(sum(y_train_un == 1)))
print("After UpSampling, counts of label 'No': {} \n".format(sum(y_train_un == 0)))


print("After UpSampling, the shape of X_train: {}".format(X_train_un.shape))
print("After UpSampling, the shape of y-train: {} \n".format(y_train_un.shape))

# @title Modeling
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

def best_model(model):
    print(model.best_score_)
    print(model.best_params_)
    print(model.best_estimator_)
def get_auc_scores(y_actual, method,method2):
    auc_score = roc_auc_score(y_actual, method);
    fpr_df, tpr_df, _ = roc_curve(y_actual, method2);
    return (auc_score, fpr_df, tpr_df)

log_primal = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=250, multi_class='auto',n_jobs=None, # Changed 'warn' to 'auto'
                                penalty='l2', random_state=None, solver='lbfgs',tol=1e-05, verbose=0, warm_start=False)
log_primal.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)

import pandas as pd
df_train = pd.read_csv('https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
numerical_cols = df_train.select_dtypes(include=['number']).columns
df_train_numeric = df_train[numerical_cols]

poly2 = PolynomialFeatures(degree=2)
df_train_pol2 = poly2.fit_transform(df_train_numeric)

# Check if 'Exited' column exists and is correctly named
print(df_train.columns)


log_pol2 = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=300, multi_class='auto', n_jobs=None,  # Changed 'warn' to 'auto'
                              penalty='l2', random_state=None, solver='liblinear',tol=0.0001, verbose=0, warm_start=False)
log_pol2.fit(df_train_pol2,df_train['Churn'])

# @title Model prediction

bccm = LogisticRegression()

X_train_transformed =  df_train.select_dtypes(include=['number']).drop('Churn', axis=1) # Use .drop() to remove the 'Churn' column
y_train = df_train.Churn
bccm.fit(X_train_transformed, y_train)


from sklearn.model_selection import train_test_split
X = df_train.select_dtypes(include=['number']).drop('Churn', axis=1)
y = df_train.Churn
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Adjust test_size and random_state as needed


X_test_transformed = X_test.copy()

y_pred = bccm.predict(X_test_transformed)
y_pred.shape

y_pred

bccm.predict_proba(X_test_transformed)

# @title Model evaluation
from sklearn.metrics import classification_report
print(classification_report(y_test, bccm.predict(X_test)))

from sklearn.metrics import classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import pandas as pd

def get_auc_scores(y_true, y_pred, y_pred_proba):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    auc_score = auc(fpr, tpr)
    return auc_score, fpr, tpr


df_test = pd.read_csv('https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')


from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier()


X_train_numerical = X_train.select_dtypes(include=['number'])
RF.fit(X_train_numerical, y_train)

X_test_numerical = df_test.select_dtypes(include=['number']).drop('Churn', axis=1)

auc_RF_test, fpr_RF_test, tpr_RF_test = get_auc_scores(df_test.Churn,
                                                       RF.predict(X_test_numerical),
                                                       RF.predict_proba(X_test_numerical)[:,1])

plt.figure(figsize = (12,6), linewidth= 1)
plt.plot(fpr_RF_test, tpr_RF_test, label = 'RF score: ' + str(round(auc_RF_test, 5)))
plt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.savefig('roc_results_ratios.png')
plt.show()

"""# **Explanation**

* The Banking sector is evolving rapidly and is very well influenced by technological advancements, changing consumer preferences,and a competitive market.
* Customer churn, which is the phenomenon of customers discontinuing their relationship with a bank, poses unique challenges and opportunities. When a bank loses customers, it can seriously affect how much money it makes and its market standing.
* Machine learning, with its predictive capabilities, offers a transformative approach to understanding and mitigating the
challenges posed by customer churn.





"""